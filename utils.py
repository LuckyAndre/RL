from itertools import product

import numpy as np
import matplotlib.pyplot as plt


def observation_int(observation):
    """
    –ø–µ—Ä–µ–≤–æ–¥ –æ—Ç–≤–µ—Ç–∞ —Å—Ä–µ–¥—ã –≤ int
    """
    return tuple([int(i) for i in observation])


def run_episode(env, pi):
    """
    –ø—Ä–æ–≥–æ–Ω –æ–¥–Ω–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞ –≤ —Å—Ä–µ–¥–µ env –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ pi —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–π–¥–µ–Ω–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ –Ω–∞–≥—Ä–∞–¥
    """
    # –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    observation = env.reset()
    states, rewards = [observation_int(observation)], [0.0]  # S0, A0, R1, S1, A1... (R0 —Å—á–∏—Ç–∞–µ–º —Ä–∞–≤–Ω—ã–º 0)

    # —à–∞–≥–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    for _ in range(1000):
        observation, reward, done, info = env.step(pi[states[-1]])
        states.append(observation_int(observation))
        rewards.append(reward)

        if done:
            break

    return states, rewards


def update_returns(R_sum, R_count, states, rewards, gamma):
    """
    –î–ª—è –æ–¥–Ω–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞ (—Å–µ—Ä–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π states) —Ä–∞—Å—á–∏—Ç—ã–≤–∞–µ–º –Ω–∞–≥—Ä–∞–¥—É —Å —É—á–µ—Ç–æ–º –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    """
    # —à–∞–≥–∞–µ–º –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ, –ø—Ä–æ–ø—É—Å—Ç–∏–≤ —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω—É—é –≤–µ—Ä—à–∏–Ω—É
    g = 0
    for t in range(len(states) - 2, -1, -1):
        g = g * gamma + rewards[t + 1]  # Gt = R(t+1) + Œ≥* R(t+2) + Œ≥^2 * R(t+3)+‚Ä¶ –¥–∞–ª–µ–µ –≤—ã—á–∏—Å–ª–∏–º vœÄ(s)=ùîºœÄ[Gt‚à£St=s]
        R_sum[states[t]] += g
        R_count[states[t]] += 1

    return R_sum, R_count


def score_pi(env, pi, episodes=100_000):
    """
    –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ pi –≤ —Å—Ä–µ–¥–µ env
    """
    total_score = 0
    for _ in range(episodes):
        states, rewards = run_episode(env, pi)
        total_score += sum(rewards)
    return total_score / episodes


def get_random_Q(Q_space_size):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª—É—á–∞–π–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ Q
    """
    Q = np.random.random(size=(Q_space_size))

    # player not usable states
    Q[0:5] = 0.0

    # player terminal states
    Q[22:] = 0.0

    # dealer not usable states
    Q[:, 0] = 0.0
    return Q


def compute_policy_by_Q(Q):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤—ã–±–∏—Ä–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º Q-—Ñ—É–Ω–∫—Ü–∏–∏
    """
    return np.argmax(Q, axis=(len(Q.shape) - 1))


def run_episode_Q_learning(env, pi, Q, alpha=0.05, epsilon=0.0, gamma=0.9):
    """
    –ü–µ—Ä–µ–æ—Ü–µ–Ω–∫–∞ —Ñ—É–Ω–∫—Ü–∏–∏ Q –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–¥–Ω–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞
    """
    nA = Q.shape[-1]

    observation = env.reset()
    s = observation_int(observation)  # S0
    a = pi[s] if np.random.rand() > epsilon else np.random.randint(nA)  # A0

    for _ in range(1000):
        observation, reward, done, info = env.step(a)  # —à–∞–≥
        s_prime = observation_int(observation)  # —Å–ª–µ–¥—É—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        if observation[3] < 0:
            print('less 0', observation, s_prime)
        a_prime = pi[s_prime] if np.random.rand() > epsilon else np.random.randint(nA)  # —Å–ª–µ–¥—É—é—â–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ
        Q[s][a] = Q[s][a] + alpha * (reward + gamma * np.max(Q[s_prime]) - Q[s][a])  # –æ—Ü–µ–Ω–∫–∞ Q —Å —É—á–µ—Ç–æ–º —Å–ª–µ–¥. —Å–æ—Å—Ç–æ—è–Ω–∏—è
        s, a = s_prime, a_prime

        if done:
            break

    return Q


def params_grid(gam_lst=[1], alp_lst=[0.5, 0.1, 0.15], eps_lst=[0.5, 0.1, 0.15]):
    """
    –°–µ—Ç–∫–∞ –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    """
    for combination in product(gam_lst, alp_lst, eps_lst):
        param = {
            'gamma': combination[0],
            'alpha': combination[1],
            'epsilon': combination[2],
        }
        yield param


def learning_loop(env, pi, Q, alpha, epsilon, gamma, total_episodes, pi_score_step):
    """
    –ò—Ç–µ—Ä–∞—Ç–∏—Ä–≤–Ω–æ —É–ª—É—á—à–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é pi –ø–æ–≤—Ç–æ—Ä–∏–≤ total_episodes —à–∞–≥–æ–≤ Q-learning —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ alpha, epsilon, gamma
    """
    # –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
    score_results = []
    score_step = []

    # —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏
    for n in range(total_episodes):

        # –æ—Ü–µ–Ω–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        if (n % pi_score_step) == 0:
            score_results.append(score_pi(env, pi, episodes=1_000))
            score_step.append(n)

        # Q-learning step
        Q = run_episode_Q_learning(env=env, pi=pi, Q=Q, alpha=alpha, epsilon=epsilon, gamma=gamma)
        pi = compute_policy_by_Q(Q)

    return {
        'pi': pi,
        'Q': Q,
        'score_step': score_step,
        'score_results': score_results,
        'alpha': alpha,
        'epsilon': epsilon,
        'gamma': gamma
    }


def plot_learning_curve(learning_results_lst):
    """
    –û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è, –ø–æ–ª—É—á–µ–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ learning_loop
    """
    # figure
    fig = plt.figure(figsize=(20, 8))
    ax = plt.axes() # Add an axes to the current figure and make it the current axes

    # data
    for l_result in learning_results_lst:
        ax.plot(
            l_result['score_step'],
            l_result['score_results'],
            label=f"alp={l_result['alpha']} eps={l_result['epsilon']} gam={l_result['gamma']}"
        )

    # annotation
    ax.set_title('Learning speed', fontsize=14, fontweight='bold')
    ax.set_xlabel('episodes')
    ax.set_ylabel('mean reward')
    ax.legend()
    plt.show(fig)


def run_episode_actions(env, pi, nA, epsilon):
    """
    –ø—Ä–æ–≥–æ–Ω –æ–¥–Ω–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞ –≤ —Å—Ä–µ–¥–µ env –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ pi —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–π–¥–µ–Ω–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π, –¥–µ–π—Å—Ç–≤–∏–π –∏ –Ω–∞–≥—Ä–∞–¥
    """

    # –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    observation = env.reset()
    s = observation_int(observation)  # S0
    a = pi[s] if np.random.rand() > epsilon else np.random.randint(nA)  # A0

    # —Å–ø–∏—Å–∫–∏ —Å —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏, –¥–µ–π—Å—Ç–≤–∏—è–º–∏, –Ω–∞–≥—Ä–∞–¥–∞–º–∏
    states, actions, rewards = [s], [a], [0]

    # —à–∞–≥–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    for _ in range(1000):
        observation, reward, done, info = env.step(a)
        s = observation_int(observation)
        states.append(s)
        a = pi[s] if np.random.rand() > epsilon else np.random.randint(nA)
        actions.append(a)
        rewards.append(reward)
        if done:
            break

    return states, actions, rewards


def update_returns_actions(R_sum, R_count, states, actions, rewards, gamma):
    """
    –î–ª—è –æ–¥–Ω–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞ (—Å–µ—Ä–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π states) —Ä–∞—Å—á–∏—Ç—ã–≤–∞–µ–º –Ω–∞–≥—Ä–∞–¥—É —Å —É—á–µ—Ç–æ–º –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    """
    # —à–∞–≥–∞–µ–º –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ, –ø—Ä–æ–ø—É—Å—Ç–∏–≤ —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω—É—é –≤–µ—Ä—à–∏–Ω—É
    g = 0
    for t in range(len(states) - 2, -1, -1):
        g = g * gamma + rewards[t + 1]  # Gt = R(t+1) + Œ≥* R(t+2) + Œ≥^2 * R(t+3)+‚Ä¶ –¥–∞–ª–µ–µ –≤—ã—á–∏—Å–ª–∏–º vœÄ(s)=ùîºœÄ[Gt‚à£St=s]
        R_sum[states[t]][actions[t]] += g
        R_count[states[t]][actions[t]] += 1

    return R_sum, R_count


def update_returns_actions_offpolicy_MC(Q, C, pi, states, actions, rewards, epsilon, gamma):
    """
    states, actions, rewards —Å–≥–µ–Ω–µ–Ω—Ä–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –º—è–≥–∫–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ pi'
    """

    # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∏–µ
    nA = Q.shape[-1]
    g = 0
    w = 1
    prob_best_action = 1 - (nA - 1) * epsilon / nA  # ???

    # –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ Q
    s = states[-1]
    a = actions[-1]
    C[s][a] = C[s][a] + w
    Q[s][a] = Q[s][a] + w / C[s][a] * (g - Q[s][a]) # —Ñ–æ—Ä–º—É–ª–∞ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å –¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ —Å—É–º–º—É –≤–µ—Å–æ–≤

    for t in range(len(states) - 2, -1, -1):

        if actions[t + 1] != pi[states[t + 1]]:  # –µ—Å–ª–∏ –¥–µ–π—Å—Ç–≤–∏–µ pi' —Ä–∞—Å—Ö–æ–¥–∏—Ç—Å—è —Å–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π pi
            break

        g = g * gamma + rewards[t + 1]
        w = w * 1 / (prob_best_action)

        # –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ Q
        s, a = states[t], actions[t]
        C[s][a] = C[s][a] + w
        Q[s][a] = Q[s][a] + w / C[s][a] * (g - Q[s][a])

    return Q, C


def params_grid_MC(gam_lst=[1], eps_lst=[0.05, 0.1, 0.3, 0.5, 0.7]):
    """
    –°–µ—Ç–∫–∞ –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    """
    for combination in product(gam_lst, eps_lst):
        param = {
            'gamma': combination[0],
            'epsilon': combination[1],
        }
        yield param


def learning_loop_MC(env, pi, Q, C, epsilon, gamma, total_episodes, pi_score_step):
    """
    –ò—Ç–µ—Ä–∞—Ç–∏—Ä–≤–Ω–æ —É–ª—É—á—à–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é pi –ø–æ–≤—Ç–æ—Ä–∏–≤ total_episodes —à–∞–≥–æ–≤ offpolicy_MC epsilon, gamma
    """
    # –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
    score_results = []
    score_step = []
    # —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏
    for n in range(total_episodes):

        # –æ—Ü–µ–Ω–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        if (n % pi_score_step) == 0:
            score_results.append(score_pi(env, pi, episodes=1_000))
            score_step.append(n)

        # offpolicy_MC step
        nA = Q.shape[-1]
        states, actions, rewards = run_episode_actions(env, pi, nA, epsilon) # –≥–µ–Ω–µ–Ω—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —ç–ø–∏–∑–æ–¥
        Q, C = update_returns_actions_offpolicy_MC(Q, C, pi, states, actions, rewards, epsilon, gamma)
        pi = compute_policy_by_Q(Q)

    return {
        'pi': pi,
        'Q': Q,
        'score_step': score_step,
        'score_results': score_results,
        'epsilon': epsilon,
        'gamma': gamma
    }


def plot_learning_curve_MC(learning_results_lst):
    """
    –û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è, –ø–æ–ª—É—á–µ–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ learning_loop_MC
    """
    # figure
    fig = plt.figure(figsize=(20, 8))
    ax = plt.axes() # Add an axes to the current figure and make it the current axes

    # data
    for l_result in learning_results_lst:
        ax.plot(
            l_result['score_step'],
            l_result['score_results'],
            label=f"eps={l_result['epsilon']} gam={l_result['gamma']}"
        )

    # annotation
    ax.set_title('Learning speed', fontsize=14, fontweight='bold')
    ax.set_xlabel('episodes')
    ax.set_ylabel('mean reward')
    ax.legend()
    plt.show(fig)